# repsly.yml - Repsly configuration (cleaned and minimal)

dag:
  dag_id: "repsly_extract_transform"
  description: "Extract from Repsly API and transform with dbt (incremental, production-ready)"
  owner: "data-team"
  schedule:
    # Schedule type for DAG execution (manual, hourly, daily, weekly, monthly, cron)
    type: "daily"
    # Time for daily/weekly/monthly schedules (HH:MM format)
    time: "03:00"
    timezone: "UTC"
  # Maximum number of concurrent DAG runs (recommended: 1 for data consistency)
  max_active_runs: 1
  start_date: "2025-07-18"
  # Number of retry attempts on task failure (0-5 recommended)
  retries: 1
  # Minutes to wait between retries
  retry_delay_minutes: 10
  email_on_failure: true
  email_on_retry: false
  email_on_success: true
  # Tags for DAG categorization and filtering in Airflow UI
  tags:
    - "repsly"
    - "extract"
    - "transform"
    - "dbt"
    - "incremental"
    - "production"

notifications:
  email:
    # Email addresses to notify on task/DAG failures
    failure_recipients:
      - "mustafa.zaki@mammoth.org"
    # Email addresses to notify on successful completion
    success_recipients:
      - "mustafa.zaki@mammoth.org"
    # Email addresses to notify on task retries
    retry_recipients:
      - "mustafa.zaki@mammoth.org"

api:
  base_url: "https://api.repsly.com/v3"
  # Test endpoint for connection validation
  test_endpoint: "export/clients/0"
  rate_limiting:
    # API requests per second to avoid rate limiting (1-10 for CRM systems)
    requests_per_second: 5
    # Number of retry attempts for failed API calls (1-5 recommended)
    max_retries: 3
    # Exponential backoff multiplier for retries (1.0-3.0 typical)
    backoff_factor: 2.0
    # Request timeout in seconds (30-120 for CRM systems)
    timeout_seconds: 60

extraction:
  # Extraction mode affects record limits and date ranges (testing, production)
  mode: "production"
  source_system: repsly
  incremental:
    # Enable incremental extraction using timestamps/offsets (true, false)
    enabled: true
    # Minutes to overlap with previous extraction to prevent data gaps (10-60 recommended)
    lookback_minutes: 30
    # Days after which to trigger full refresh instead of incremental (30-365)
    full_refresh_days: 90
    # File path for storing extraction state/watermarks
    state_path: "/opt/airflow/state/repsly_watermarks.json"
    initial_extraction:
      # Set this to when you want data extraction to start from (YYYY-MM-DD)
      start_date: "2025-01-01"
      # Reset all watermarks on next run - use once then set to false (true, false)
      reset_watermarks: false
      # Strategy for large initial extractions (chunked, full)
      backfill_strategy: "chunked"
      # Days per chunk for initial backfill (7-60 recommended)
      backfill_chunk_days: 30
  testing:
    # Maximum records per endpoint in testing mode (10-1000)
    max_records_per_endpoint: 100
    # Date range for testing extractions in days (1-30)
    date_range_days: 7
  production:
    # Maximum records per endpoint in production (null for unlimited)
    max_records_per_endpoint: null
    # Date range for initial production extraction in days (30-365)
    date_range_days: 365
  paths:
    # Directory for temporary raw data storage
    raw_data_directory: "/opt/airflow/data/raw/repsly"

  endpoints:
    # Core endpoints that must always work - highest priority
    always_extract:
      - "clients"          # Highest priority - other data depends on this
      - "client_notes"     # High priority - main business data
    
    # Endpoints that can be optionally enabled/disabled
    optional_extract:
      - "daily_working_time"
      - "visits"
      - "representatives"  
      - "users"
      - "purchase_orders"
      - "forms"
      - "photos"
      - "visit_schedules"
      - "visit_schedules_extended"
      - "visit_schedule_realizations"
    
    # Endpoints to completely skip (move from always/optional to here)
    disabled: []
      
    # Define which endpoints must complete before others can start
    dependencies:
      client_notes:
        depends_on: ["clients"]
        # Execution priority within dependency group (high, medium, low)
        priority: "high"
      visits:
        depends_on: ["clients", "representatives"]  
        priority: "medium"
      daily_working_time:
        depends_on: ["representatives"]
        priority: "low"

warehouse:
  # Active warehouse type for data loading (clickhouse, snowflake, bigquery)
  active_warehouse: "clickhouse"
  schemas:
    # Schema for raw/bronze layer data
    bronze_schema: "bronze_repsly"
    # Schema for transformed/silver layer data
    silver_schema: "silver_repsly"
    # Alias for compatibility
    raw_schema: "bronze_repsly"
    # Alias for dbt staging models
    staging_schema: "silver_repsly"
  
  clickhouse:
    # Batch size for bulk inserts (100-5000 for CRM data)
    chunk_size: 1000
    # Number of concurrent connections (1-10)
    connection_pool_size: 5
    # Retry attempts for failed warehouse operations (1-5)
    max_retries: 3
    # Table partitioning strategy (by_date, by_month, none)
    partition_strategy: "by_date"

dbt:
  # Path to dbt project directory
  project_dir: "/opt/airflow/dbt"
  # Path to dbt profiles directory
  profiles_dir: "/opt/airflow/dbt"
  execution:
    # Stop dbt run on first model failure (true, false)
    fail_fast: true
    # Force full refresh of incremental models (true, false)
    full_refresh: false
    # Number of concurrent dbt threads (1-8 recommended)
    threads: 2