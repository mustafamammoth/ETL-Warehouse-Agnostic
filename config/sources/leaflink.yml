# leaflink.yml - LeafLink configuration (cleaned and minimal)

dag:
  dag_id: "leaflink_extract_transform"
  description: "Extract from LeafLink API and transform with dbt (incremental, production-ready)"
  owner: "data-team"
  schedule:
    # Schedule type for DAG execution (manual, hourly, daily, weekly, monthly, cron)
    type: cron
    # Cron expression for custom scheduling
    cron_expression: "0 * * * *"
    # Time for daily/weekly/monthly schedules (HH:MM format)
    time: "04:00"
    timezone: "UTC"
  # Maximum number of concurrent DAG runs (recommended: 1 for data consistency)
  max_active_runs: 1
  start_date: "2025-06-30"
  # Number of retry attempts on task failure (0-5 recommended)
  retries: 1
  # Minutes to wait between retries
  retry_delay_minutes: 10
  email_on_failure: true
  email_on_retry: false
  email_on_success: true
  # Tags for DAG categorization and filtering in Airflow UI
  tags:
    - "leaflink"
    - "extract"
    - "transform"
    - "dbt"
    - "incremental"
    - "production"
    - "orders"

notifications:
  email:
    # Email addresses to notify on task/DAG failures
    failure_recipients:
      - "mustafa.zaki@mammoth.org"
    # Email addresses to notify on successful completion
    success_recipients:
      - "mustafa.zaki@mammoth.org"
    # Email addresses to notify on task retries
    retry_recipients:
      - "mustafa.zaki@mammoth.org"

api:
  base_url: "https://app.leaflink.com/api/v2"
  rate_limiting:
    # API requests per second to avoid rate limiting (1-20 typical range)
    requests_per_second: 10
    # Number of retry attempts for failed API calls (1-5 recommended)
    max_retries: 3
    # Exponential backoff multiplier for retries (1.0-3.0 typical)
    backoff_factor: 2.0
    # Request timeout in seconds (30-120 recommended)
    timeout_seconds: 60

extraction:
  # Extraction mode affects record limits and date ranges (testing, production)
  mode: "production"
  source_system: leaflink
  incremental:
    # Enable incremental extraction using timestamps/offsets (true, false)
    enabled: true
    # Minutes to overlap with previous extraction to prevent data gaps (5-60 recommended)
    lookback_minutes: 30
    # Days after which to trigger full refresh instead of incremental (30-365)
    full_refresh_days: 90
    # File path for storing extraction state/watermarks
    state_path: "/opt/airflow/state/leaflink_watermarks.json"
    initial_extraction:
      # Reset all watermarks on next run - use once then set to false (true, false)
      reset_watermarks: false
      # Strategy for large initial extractions (chunked, full)
      backfill_strategy: "chunked"
      # Days per chunk for initial backfill (7-60 recommended)
      backfill_chunk_days: 30
  testing:
    # Maximum records per endpoint in testing mode (10-1000)
    max_records_per_endpoint: 100
    # Date range for testing extractions in days (1-30)
    date_range_days: 7
  production:
    # Maximum records per endpoint in production (null for unlimited)
    max_records_per_endpoint: null
    # Date range for initial production extraction in days (30-365)
    date_range_days: 365
  paths:
    # Directory for temporary raw data storage
    raw_data_directory: "/opt/airflow/data/raw/leaflink"

  endpoints:
    # Endpoints that always run in every extraction
    always_extract:
      - orders_received
      - line_items
      - products
      - product_categories
      - product_subcategories
      - product_batches
      - batch_documents
      - companies
      - company_staff
      - licenses
      - license_types
      - brands
      - promocodes
      - reports
      - customers
      - contacts
            
    # Endpoints that can be optionally enabled/disabled
    optional_extract:
      - order_sales_reps
      - order_payments
      - product_lines
      - product_images
      - strains
      - activity_entries
      - customer_status
      - customer_tiers
      - order_event_logs
    
    # Endpoints to completely skip (move from always/optional to here)
    disabled: []
      
    dependencies:
      # Define which endpoints must complete before others can start
      order_payments:
        depends_on: ["orders_received"]
        # Execution priority within dependency group (high, medium, low)
        priority: "medium"

warehouse:
  # Active warehouse type for data loading (clickhouse, snowflake, bigquery)
  active_warehouse: "clickhouse"
  schemas:
    # Schema for raw/bronze layer data
    bronze_schema: "bronze_leaflink"
    # Schema for transformed/silver layer data
    silver_schema: "silver_leaflink"
    # Alias for dbt staging models
    staging_schema: "silver_leaflink"
  
  clickhouse:
    # Batch size for bulk inserts (100-10000)
    chunk_size: 1000
    # Number of concurrent connections (1-10)
    connection_pool_size: 5
    # Retry attempts for failed warehouse operations (1-5)
    max_retries: 3
    # Table partitioning strategy (by_date, by_month, none)
    partition_strategy: "by_date"

companies:
  # Each company gets its own schema and can have different schedules
  - company_id: 20018
    # Region identifier for logging and organization
    region: ny
    # ClickHouse schema for this company's raw data
    raw_schema: bronze_leaflink_ny
    # Cron schedule for this company's extractions
    schedule_cron: "0 * * * *"
  - company_id: 24443
    region: nj
    raw_schema: bronze_leaflink_nj
    schedule_cron: "0 * * * *"
  - company_id: 5423
    region: ca
    raw_schema: bronze_leaflink_ca
    schedule_cron: "0 * * * *"

dbt:
  # Path to dbt project directory
  project_dir: "/opt/airflow/dbt"
  # Path to dbt profiles directory
  profiles_dir: "/opt/airflow/dbt"
  execution:
    # Stop dbt run on first model failure (true, false)
    fail_fast: true
    # Force full refresh of incremental models (true, false)
    full_refresh: false
    # Number of concurrent dbt threads (1-8 recommended)
    threads: 2